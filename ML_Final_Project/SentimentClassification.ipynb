{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2da9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seanhuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/seanhuang/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "#library that contains punctuation\n",
    "import string\n",
    "#defining function for tokenization\n",
    "import re\n",
    "#importing nlp library\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed2bcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(370454, 14) \n",
      "\n",
      "(370451, 14) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd_pledged_real</th>\n",
       "      <th>usd_goal_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1533.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000003930</td>\n",
       "      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>2017-09-02 04:43:57</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>15</td>\n",
       "      <td>United States</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>30000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>United States</td>\n",
       "      <td>220.0</td>\n",
       "      <td>45000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>14</td>\n",
       "      <td>United States</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>19500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370449</th>\n",
       "      <td>999976400</td>\n",
       "      <td>ChknTruk Nationwide Charity Drive 2014 (Canceled)</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2014-09-17 02:35:30</td>\n",
       "      <td>25.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370450</th>\n",
       "      <td>999977640</td>\n",
       "      <td>The Tribe</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2011-07-19</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2011-06-22 03:35:14</td>\n",
       "      <td>155.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>5</td>\n",
       "      <td>United States</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370451</th>\n",
       "      <td>999986353</td>\n",
       "      <td>Walls of Remedy- New lesbian Romantic Comedy f...</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2010-08-16</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>2010-07-01 19:40:30</td>\n",
       "      <td>20.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370452</th>\n",
       "      <td>999987933</td>\n",
       "      <td>BioDefense Education Kit</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Technology</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-02-13</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>2016-01-13 18:13:53</td>\n",
       "      <td>200.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>6</td>\n",
       "      <td>United States</td>\n",
       "      <td>200.0</td>\n",
       "      <td>15000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370453</th>\n",
       "      <td>999988282</td>\n",
       "      <td>Nou Renmen Ayiti!  We Love Haiti!</td>\n",
       "      <td>Performance Art</td>\n",
       "      <td>Art</td>\n",
       "      <td>USD</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2011-07-19 09:07:47</td>\n",
       "      <td>524.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>17</td>\n",
       "      <td>United States</td>\n",
       "      <td>524.0</td>\n",
       "      <td>2000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370451 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               name  \\\n",
       "0       1000002330                    The Songs of Adelaide & Abullah   \n",
       "1       1000003930      Greeting From Earth: ZGAC Arts Capsule For ET   \n",
       "2       1000004038                                     Where is Hank?   \n",
       "3       1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "4       1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "...            ...                                                ...   \n",
       "370449   999976400  ChknTruk Nationwide Charity Drive 2014 (Canceled)   \n",
       "370450   999977640                                          The Tribe   \n",
       "370451   999986353  Walls of Remedy- New lesbian Romantic Comedy f...   \n",
       "370452   999987933                           BioDefense Education Kit   \n",
       "370453   999988282                  Nou Renmen Ayiti!  We Love Haiti!   \n",
       "\n",
       "               category main_category currency    deadline     goal  \\\n",
       "0                Poetry    Publishing      GBP  2015-10-09   1000.0   \n",
       "1        Narrative Film  Film & Video      USD  2017-11-01  30000.0   \n",
       "2        Narrative Film  Film & Video      USD  2013-02-26  45000.0   \n",
       "3                 Music         Music      USD  2012-04-16   5000.0   \n",
       "4          Film & Video  Film & Video      USD  2015-08-29  19500.0   \n",
       "...                 ...           ...      ...         ...      ...   \n",
       "370449      Documentary  Film & Video      USD  2014-10-17  50000.0   \n",
       "370450   Narrative Film  Film & Video      USD  2011-07-19   1500.0   \n",
       "370451   Narrative Film  Film & Video      USD  2010-08-16  15000.0   \n",
       "370452       Technology    Technology      USD  2016-02-13  15000.0   \n",
       "370453  Performance Art           Art      USD  2011-08-16   2000.0   \n",
       "\n",
       "                   launched  pledged   state  backers         country  \\\n",
       "0       2015-08-11 12:12:28      0.0  failed        0  United Kingdom   \n",
       "1       2017-09-02 04:43:57   2421.0  failed       15   United States   \n",
       "2       2013-01-12 00:20:50    220.0  failed        3   United States   \n",
       "3       2012-03-17 03:24:11      1.0  failed        1   United States   \n",
       "4       2015-07-04 08:35:03   1283.0  failed       14   United States   \n",
       "...                     ...      ...     ...      ...             ...   \n",
       "370449  2014-09-17 02:35:30     25.0  failed        1   United States   \n",
       "370450  2011-06-22 03:35:14    155.0  failed        5   United States   \n",
       "370451  2010-07-01 19:40:30     20.0  failed        1   United States   \n",
       "370452  2016-01-13 18:13:53    200.0  failed        6   United States   \n",
       "370453  2011-07-19 09:07:47    524.0  failed       17   United States   \n",
       "\n",
       "        usd_pledged_real  usd_goal_real  \n",
       "0                    0.0        1533.95  \n",
       "1                 2421.0       30000.00  \n",
       "2                  220.0       45000.00  \n",
       "3                    1.0        5000.00  \n",
       "4                 1283.0       19500.00  \n",
       "...                  ...            ...  \n",
       "370449              25.0       50000.00  \n",
       "370450             155.0        1500.00  \n",
       "370451              20.0       15000.00  \n",
       "370452             200.0       15000.00  \n",
       "370453             524.0        2000.00  \n",
       "\n",
       "[370451 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Reading\n",
    "df = pd.read_csv('ks_df.csv')\n",
    "print(df.shape, \"\\n\")\n",
    "df = df.dropna(subset=['name']) # sentiment classification for column 'name'\n",
    "print(df.shape, \"\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6438ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text (join into one string)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split('W+',text)\n",
    "    return tokens\n",
    "\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b58d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Progress: 370450\r"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "# storing the puntuation free text\n",
    "df['name']= df['name'].apply(lambda x:remove_punctuation(x))\n",
    "# lowercase everything\n",
    "df['name']= df['name'].apply(lambda x: x.lower())\n",
    "#applying function to the column\n",
    "df['name']= df['name'].apply(lambda x: tokenization(x))\n",
    "#applying the function\n",
    "df['name']= df['name'].apply(lambda x:remove_stopwords(x))\n",
    "# stemming\n",
    "df['name']=df['name'].apply(lambda x: stemming(x))\n",
    "# lemmatizing\n",
    "df['name']=df['name'].apply(lambda x:lemmatizer(x))\n",
    "\n",
    "def sentiment(model, text):\n",
    "    global i \n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    negative = scores[0]\n",
    "    neutral = scores[1]\n",
    "    positive = scores[2]\n",
    "    print(\"Current Progress:\", i, end='\\r')\n",
    "    i += 1\n",
    "\n",
    "    return negative, neutral, positive\n",
    "\n",
    "df['negative'], df['neutral'], df['positive'] = zip(*df['name'].apply(lambda x: sentiment(model, x)))\n",
    "df.to_csv('final_ks.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
